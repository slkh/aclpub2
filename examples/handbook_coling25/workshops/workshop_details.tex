\chapter{Workshop Details}
\subsection{Sunday, January 19, 2025}
\noindent{\textbf{Workshop 1: The 1st Workshop on NLP for Languages Using Arabic Script (AbjadNLP 2025)}}\\  
\noindent{\emph{Organizers: Mo El-Haj, Hugh Paterson III, Saad Ezzini, Ignatius Ezeani, Mahum Hayat Khan, Muhammad Sharjeel, Sina Ahmadi, Cynthia Amol, Amal Haddad Haddad, Jaleh Delfani, Ruslan Mitkov, and Paul Rayson}}\\

\noindent{AbjadNLP is dedicated to advancing innovation and gaining deeper insights into Natural Language Processing (NLP) for languages that use the Arabic script. Our primary focus is on Abjad and Ajami languages that utilise the Arabic script or its variations. Traditionally associated with Semitic languages, Abjad scripts represent consonants in every syllable. In contrast, Ajami scripts denote the alphabetic use of the Arabic script in various African contexts, representing non-Arabic languages. We are interested in research on languages that fall under the Abjad or Ajami categories that use the Arabic script or any variations of it.} \\

\noindent{\url{https://wp.lancs.ac.uk/abjad/}} \\
\noindent{Time: 09:00-16:45}\\
\noindent{Location: Conf. Hall B Room D}\\

\noindent\hrulefill

\noindent{\textbf{Workshop 2: CHiPSAL: Challenges in Processing South Asian Languages}}\\  
\noindent{\emph{Organizers: Kengatharaiyer Sarveswaran, Ashwini Vaidya, Bal Krishna Bal, Sana Shams, and Surendrabikram Thapa}}\\

\noindent{South Asia, consisting of Afghanistan, Bangladesh, Bhutan, India, Maldives, Nepal, Pakistan, and Sri Lanka, is one of the most populous regions in the world, with approximately 1.97 billion people. Home to over 700 languages and around 25 major scripts, the region reflects a rich cultural and linguistic heritage. Additionally, more than 50 million South Asians live abroad. Despite this diversity, South Asian languages are underrepresented in language technology. Recent large language models (LLMs) include minimal data from this region, and the challenges in processing South Asian languages begin with encoding issues. While most scripts are encoded in Unicode, some applications may not render them correctly due to orthographic complexities, and language input remains a problem in the region. The linguistic intricacies of these languages, with multiple writing systems and long literary traditions, further complicate natural language processing tasks. Dialectal and cultural variations, as well as close language contact, add an extra level of complexity. Therefore, this workshop focuses on the challenges in processing South Asian languages, covering issues related to linguistic and cultural aspects, encoding and orthography, and resource constraints. By addressing these challenges, we aim to facilitate South Asian language processing with a focus on linguistic and cultural heritage.} \\

\noindent{\url{https://sites.google.com/view/chipsal/}} \\
\noindent{Time: 08:30-15:45}\\
\noindent{Location: Online Virtual Only}\\

\noindent\hrulefill

\noindent{\textbf{Workshop 3: Context and Meaning: Navigating Disagreements in NLP Annotation (CM-ND-NLP)}}\\
\noindent{\emph{Organizers: Michael Roth and Dominik Schlechtweg}}\\

\noindent{Disagreements among annotators pose a significant challenge in Natural Language Processing, impacting the quality and reliability of datasets and consequently the performance of NLP models. This workshop aims to explore the complexities of annotation disagreements, their causes, and strategies towards their effective resolution, with a focus on meaning in context.} \\

\noindent{\url{https://comedinlp.github.io/}} \\
\noindent{Time: 09:30-16:10}\\
\noindent{Location: Capital Suite 6 (Level 2)}\\

\noindent\hrulefill

\noindent{\textbf{Workshop 4: New Horizons in Computational Linguistics for Religious Texts (Coling-Rel)}}\\ 
\noindent{\emph{Organizers: Majdi Sawalha, Sane Yagi, Faisal Alshargi, Abdallah Al-Shdaifat, Ashraf Elnagar, Bayan Abu Shawar, and Noorhan Abbas}}\\

\noindent{The COLLING 2025 Workshop on New Horizons in Computational Linguistics for Religious Texts will be held with the 31st edition of COLING in 2025 in Abu Dhabi (UAE) (COLING 2025). This workshop invites researchers exploring the intersection of language technology and religious texts. This workshop aims to foster discussion on cutting-edge applications of Natural Language Processing (NLP) to religious texts, including: (i) Analyzing faith-defining canons and authoritative interpretations, (ii) Extracting insights from sermons, liturgy, prayers, and poetry, and (iii) Leveraging Large Language Models for novel research avenues. We will explore the potential of NLP to unlock new understandings of religious traditions and chart the future of this exciting research area. The workshop welcomes researchers from computational linguistics, digital humanities, and related fields.} \\

\noindent{\url{https://668b0f080fa32.site123.me/}} \\
\noindent{Time: 09:00-19:00}\\
\noindent{Location: Capital Suite 10 (Level 2)}\\

\noindent\hrulefill

\noindent{\textbf{Workshop 5: 1st Workshop on Computational Humor (CompHum)}}\\ 
\noindent{\emph{Organizers: Christian F. Hempelmann, Julia Rayz, Tiansi Dong, and Tristan Miller}}\\

\noindent{The impressive recent progress in generative AI has enabled new approaches to complex tasks that can be thought of as essentially human, such as producing or understanding humor. For its part, humor research has become a mature, interdisciplinary field, both in theoretical advances across disciplines such as psychology, linguistics, and sociology, and in a breadth of purview and empirical support. Our workshop aims to foster further work on modeling the processes of humor with current methods in computational linguistics and natural language processing, against the theoretical backdrop of humor research and with reference to relevant corpora of textual, visual, and multimodal materials.} \\

\noindent{\url{https://chum2025.github.io/}} \\
\noindent{Time: 08:45-17:40}\\
\noindent{Location: Online Virtual Only}\\

\noindent\hrulefill

\noindent{\textbf{Workshop 6: COLING 2025 Workshop on Detecting AI Generated Content (DAIGenC)}}\\ 
\noindent{\emph{Organizers: Firoj Alam, Preslav Nakov, Nizar Habash, Iryna Gurevych, Shammur Chowdhury, Artem Shelmanov, Yuxia Wang, Ekaterina Artemova, Mucahid Kutlu, and George Mikros}}\\

\noindent{This workshop aims to address the critical aspects of detecting generative content, encompassing text, images, and multimodal interactions. The emergence of advanced generative models has made it increasingly difficult to distinguish between human-written and machine-generated content, raising concerns in areas such as misinformation, copyright infringement, and the authenticity of digital media.} \\

\noindent{\url{https://genai-content-detection.gitlab.io/}} \\
\noindent{Time: 09:50-16:30}\\
\noindent{Location: Capital Suite 9 (Level 2)}\\

\noindent\hrulefill


\noindent{\textbf{Workshop 7: Workshop on Generative AI and Knowledge Graphs (GenAIK)}}\\ 
\noindent{\emph{Organizers: Genet Asefa Gesese, FIZ Karlsruhe, Harald Sack, Heiko Paulheim, Albert Meroño-Peñuela, and Lihu Chen}}\\


% Generative Artificial Intelligence (GenAI) is a branch of artificial intelligence capable of creating seemingly new, meaningful content, including text, images, and audio. It utilizes deep learning models, such as Large Language Models (LLMs), to recognize and replicate data patterns, enabling the generation of human-like content. Notable families of LLMs include GPT (GPT-3.5, GPT-3.5 Turbo, and GPT-4), LLaMA (LLaMA and LLaMA-2), and Mistral (Mistral and Mixtral). GPT, which stands for Generative Pretrained Transformer, is especially popular for text generation and is widely used in applications like ChatGPT. GenAI has taken the world by storm and revolutionized various industries, including healthcare, finance, and entertainment. However, GenAI models have several limitations, including biases from training data, generating factually incorrect information, and difficulty in understanding complex content. Additionally, their performance can vary based on domain specificity.

\noindent{In recent times, Knowledge Graphs (KGs) have attracted considerable attention for their ability to represent structured and interconnected information, and adopted by many companies in various domains. KGs represent knowledge by depicting relationships between entities, known as facts, usually based on formal ontological models. Consequently, they enable accuracy, decisiveness, interpretability, domain-specific knowledge, and evolving knowledge in various AI applications. The intersection between GenAI and KG has ignited significant interest and innovation in Natural Language Processing (NLP). For instance, by integrating LLMs with KGs during pre-training and inference, external knowledge can be incorporated for enhancing the model’s capabilities and improving interpretability. When integrated, they offer a robust approach to problemsolving in diverse areas such as information enrichment, representation learning, conversational AI, cross-domain AI transfer, bias, content generation, and semantic understanding. This workshop, aims to reinforce the relationships between Deep Learning, Knowledge Graphs, and NLP communities and foster inter-disciplinary research in the area of GenAI.} \\

\noindent{\url{https://genetasefa.github.io/GenAIK2025/}} \\
\noindent{Time: 08:45-17:40}\\
\noindent{Location: Capital Suite 8 (Level 2)}\\

\noindent\hrulefill


\noindent{\textbf{Workshop 8: The First Workshop on Multilingual Counterspeech Generation (MCG)}}\\ 
\noindent{\emph{Organizers: Rodrigo Agerri, Helena Bonaldi, Marco Guerini, María Teresa Martín-Valdivia, Arturo Montejo-Ráez, Aitor Soroa, María Estrella Vallecillo-Rodríguez, and Irune Zubiaga}}\\


% As mentioned above, research on automatic generation has mostly focused on the collection and generation of contraphones for English, but there have also been some efforts to develop contraphone datasets for Italian, French, Spanish  and Basque, which can be used to facilitate research on automatic counter-speech generation from a multilingual point of view. 

% Thus, this workshop aims to test monolingual and multilingual LLMs in particular and Language Technology in general to automatically generate counterspeech not only in English but also in languages with fewer resources. In this sense, an important goal of the workshop will be to understand the impact of using LLMs, considering for example how to deal with pressing issues such as biases, data scarcity, hallucinated content or data contamination.

\noindent{While interest in automatic approaches to Counterspeech generation has been steadily growing, including studies on data curation, detection, and generation, the large majority of the published experimental work on automatic Counterspeech generation has been carried out for English. This is due to the scarcity of both non-English manually curated training data and to the crushing predominance of English in the generative Large Language Models (LLMs) ecosystem. A workshop on exploring Multilingual Counterspeech Generation is proposed to promote and encourage research on multilingual approaches for this challenging topic.} \\

\noindent{\url{https://sites.google.com/view/multilang-counterspeech-gen/}} \\
\noindent{Time: Check online for Schedule}\\
\noindent{Location: Capital Suite 13 (Level 2)}\\

\noindent\hrulefill


\noindent{\textbf{Workshop 9: VarDial 2025: The Twelfth Workshop on NLP for Similar Languages, Varieties and Dialects}}\\ 
\noindent{\emph{Organizers: Yves Scherrer, Tommi Jauhiainen, Nikola Ljubešić, Preslav Nakov, Jörg Tiedemann, and Marcos Zampieri}}\\

\noindent{VarDial is a well-established series of workshops promoting a forum for scholars working on a range of topics related to the study of diatopic language variation from a computational perspective. The workshop deals with computational methods and language resources for closely related languages, language varieties, and dialects. We welcome papers dealing with one or more of the following topics: (i) Corpora, resources, and tools for similar languages, varieties and dialects; (ii) Adaptation of tools (taggers, parsers) for similar languages, varieties and dialects; (iii) Evaluation of language resources and tools when applied to language varieties; (iv) Reusability of language resources in NLP applications (e.g., for machine translation, POS tagging, syntactic parsing, etc.); (v) Corpus-driven studies in dialectology and language variation; (vi) Computational approaches to mutual intelligibility between dialects and similar languages; (vii) Automatic identification of lexical variation; (viii) Automatic classification of language varieties; (ix) Text similarity and adaptation between language varieties; (x) Linguistic issues in the adaptation of language resources and tools (e.g., semantic discrepancies, lexical gaps, false friends); (xi) Machine translation between closely related languages, language varieties and dialects.} \\

\noindent{\url{https://sites.google.com/view/vardial-2025}} \\
\noindent{Time: 09:00-17:30}\\
\noindent{Location: Conf. Hall B Room A}\\

\subsection{Sunday-Monday, January 19-20, 2025}

\noindent{\textbf{Workshop 10: The Joint Workshop of the 9th Financial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal)}}\\ 

\noindent{\emph{Organizers: Chung-Chi Chen, Qianqian Xie, Jimin Huang, Sophia Ananiadou, Hsin-Hsi Chen, Paloma Martínez, Doaa Samy, Mo El-Haj, Paul Rayson, Xiao-Yang (Yanglet) Liu, Benyou Wang, Hao Wang, Jordan Suchow, Alejandro Lopez-Lira, Eghbal Rahimikia, Min Peng, Hen-Hsen Huang, and Hiroya Takamura}}\\

\noindent{The joint workshop of FinNLP, FNP, and LLMFinLegal aims to explore the intersection of Natural Language Processing (NLP), Machine Learning (ML), and Large Language Models (LLMs) within the financial and legal domains. By merging the expertise and focus areas of these three workshops, we intend to foster interdisciplinary research and innovation, addressing the multifaceted challenges inherent in these fields.} \\

\noindent{\url{https://sites.google.com/nlg.csie.ntu.edu.tw/finnlp-fnp-llmfinlegal/}} \\
\noindent{Time: 09:00-17:10}\\
\noindent{Location: Conf. Hall B Room B }\\

\subsection{Monday, January 20, 2025}

\noindent{\textbf{Workshop 11: The Second COLING Workshop on Bridging Neurons and Symbols for NLP and Knowledge Graph Reasoning (BNS-KGR)}}\\ 
\noindent{\emph{Organizers: Kang Liu, Yangqiu Song, Zhen Han, Rafet Sifa, Shizhu He, and Yunfei Long}}\\

\noindent{Recent exploration shows that LLMs, e.g., ChatGPT, may pass the Turing test in human-like chatting but have limited capability even for simple reasoning tasks. It remains unclear whether LLMs reason or not. Human reasoning has been characterized as a dual-process phenomenon or as mechanisms of fast and slow thinking. These findings suggest two directions for exploring neural reasoning: starting from existing neural networks to enhance the reasoning performance with the target of symbolic-level reasoning, and starting from symbolic reasoning to explore its novel neural implementation. These two directions will ideally meet somewhere in the middle and will lead to representations that can act as a bridge for novel neural computing, which qualitatively differs from traditional neural networks, and for novel symbolic computing, which inherits the good features of neural computing. Hence the name of our workshop, with a focus on Natural Language Processing and Knowledge Graph reasoning. This workshop promotes research in both directions, particularly seeking novel proposals from the second direction.} \\

\noindent{\url{https://neusymbridge.github.io/}} \\
\noindent{Time: 08:50-18:00}\\
\noindent{Location: Conf. Hall B Room A}\\

\noindent\hrulefill

\noindent{\textbf{Workshop 12: 18th Workshop on Building and Using Comparable Corpora (BUCC)}}\\ 
\noindent{\emph{Organizers: Serge Sharoff, Ayla Rigouts Terryn, Pierre Zweigenbaum, and Reinhard Rapp}}\\

\noindent{In the language engineering and linguistics communities, research in comparable corpora has been motivated by two main reasons. In language engineering, on the one hand, it is chiefly motivated by the need to use comparable corpora as training data for statistical NLP applications such as statistical and neural machine translation or cross-lingual retrieval. In linguistics, on the other hand, comparable corpora are of interest because they enable cross-language discoveries and comparisons. It is generally accepted in both communities that comparable corpora consist of documents that are comparable in content and form in various degrees and dimensions across several languages. Parallel corpora are on the one end of this spectrum, and unrelated corpora are on the other. In recent years, the use of comparable corpora for pre-training Large Language Models (LLMs) has led to their impressive multilingual and cross-lingual abilities, which are relevant to a range of applications, including Information Retrieval, Machine Translation, Cross-lingual text classification, etc. The linguistic definitions and observations related to comparable corpora can improve methods to mine such corpora for applications of statistical NLP, for example, to extract parallel corpora from comparable corpora for neural MT or to improve cross-lingual transfer of LLMs. As such, it is of great interest to bring together builders and users of such corpora.} \\

\noindent{\url{https://comparable.lisn.upsaclay.fr/bucc2025/}} \\
\noindent{Time: 09:15-17:45}\\
\noindent{Location: Capital Suite 9 (Level 2)}\\

\noindent\hrulefill

\noindent{\textbf{Workshop 13: The Fifth Celtic Language Technology Workshop (CLT)}}\\ 
\noindent{\emph{Organizers: Brian Davis, Theodorus Fransen, Elaine Uí Dhonnchadha, Abigail Walsh, and Jane Adkins}}\\


% In Classical times, Celtic languages were found across a wide swathe of modern Eurasia. Today, they are spoken in regions of the UK and Ireland, as well as in Brittany, France. The modern languages are: Irish, Breton, Manx, Welsh, Cornish and Scottish Gaelic. Although their hereditary communities are small compared to those of most other European languages, they continue to have a vibrant presence in their traditional areas as well as in urban centres. While Irish is the only Celtic language that has official EU language status (since 2007), Welsh, Gaelic and Manx have co-official status. Breton and Cornish also have some limited status in their home regions. That said, all Celtic languages face the same issue in lacking NLP resources to ensure continued technology support in the digital era.

% While the Celtic languages share certain aspects of their sociolinguistic situation with other minority languages, their common linguistic features (e.g. VSO word order, initial mutations and reasonably complex morphology) also present unique challenges for the development of robust NLP tools. By gathering researchers from all of the Celtic languages, CLTW aims to share best practice in overcoming these difficulties.

\noindent{The CLTW community and workshop – inaugurated at COLING (Dublin) in 2014 – has become a critical focus and forum for researchers working in natural language processing (NLP) and language technologies for Celtic languages. In particular, it has galvanised and catalysed research by facilitating communication and collaboration internationally. Our community is interested in language technology for both contemporary and historical stages of the Celtic languages.} \\

\noindent{\url{https://cltworkshop.github.io/}} \\
\noindent{Time: 09:00-13:25}\\
\noindent{Location: Online Virtual Only}\\

\noindent\hrulefill

\noindent{\textbf{Workshop 14: Workshop of Evaluation of Multimodal Generation (EvalMG)}}\\ 
\noindent{\emph{Organizers: Wei Emma Zhang, Xiang Dai, Desmond Elliot, Byron Fang, Haojie Zhuang, Mong Yuan Sim, and Weitong Chen}}\\

\noindent{Multimodal generation techniques have opened new avenues for creative content generation. However, evaluating the quality of multimodal generation remains underexplored and some key questions are unanswered, such as the contributions of each modal, the utility of pre-trained large language models for multimodal generation, and measuring faithfulness and fairness in multimodal outputs. This workshop aims to foster discussions and research efforts by bringing together researchers and practitioners in natural language processing, computer vision, and multimodal AI. Our goal is to establish evaluation methods for multimodal research and advance research efforts in this direction.} \\

\noindent{\url{https://evalmg.github.io/}} \\
\noindent{Time: Check online for Schedule}\\
\noindent{Location: Capital Suite 10 (Level 2) }\\

\noindent\hrulefill

\noindent{\textbf{Workshop 15: IndoNLP: The First Workshop on Natural Language Processing for Indo-Aryan and Dravidian Languages}}\\ 
\noindent{\emph{Organizers: Ruvan Weerasinghe, Isuri Anuradha, Deshan Sumanathilaka, Mo El-Haj, Chamila Liyanage, Fahad Khan, Andrew Hardie, Asim Abbas, Ruslan Mitkov, Julian Hough, Nicholas Micallef, and Naomi Krishnarajah}}\\



% The topics of the workshop include, but are not limited to:
% \begin{itemize}
%     \item Large Language Models for Indo-Aryan Languages and Dravidian Languages.
%     \item Developing a cleaned Indo-Aryan and Dravidian language corpora (UNICODE) and digital linguistic resources.
%     \item Machine Translation and Cross-Lingual Systems
%     \item Speech Technologies: Recognition and Synthesis
%     \item Language Identification and Dialect Detection
%     \item Information Extraction, OCR systems and Knowledge Modelling
%     \item NLP Applications - Fake News, Spam, and Rumor Detection
%     \item Hate speech and Offensive Language Detection
%     \item Sentiment Analysis and Text Summarisation
%     \item NLP applications: Misinformation, Conspiracy theories. Rumours, SPAM, Phishing, and similar applications.
% \end{itemize}

\noindent{The rapid advancement of Natural Language Processing (NLP) and Large Language Models (LLMs) has transformed the landscape of computational linguistics. However, Indo-Aryan and Dravidian Languages (IADL), which represent a significant portion of South Asia's linguistic heritage, remain under-resourced and under-researched in these technological developments. This workshop aims to bridge this gap by bringing together researchers, linguists, and technologists to focus on the unique challenges and opportunities. Participants will explore innovative methods for creating and annotating digital corpora, develop speech and language technologies suited to IADL, and promote interdisciplinary collaborations. By leveraging LLMs, we seek to address the complexities of syntax, morphology, and semantics in these languages to enhance the performance of NLP applications. Furthermore, the workshop will provide a platform for sharing best practices, tools, and resources, enhancing the digital infrastructure necessary for language preservation. Through collaborative efforts, we aim to build a research community to advance NLP for IADL, contributing to linguistic diversity and cultural preservation in the digital age.} \\

\noindent{\url{https://indonlp-workshop.github.io/IndoNLP-Workshop/}} \\
\noindent{Time: 08:45-17:00}\\
\noindent{Location: Conf. Hall B Room D}\\

\noindent\hrulefill



\noindent{\textbf{Workshop 16: The First Workshop on Language Models for Low-Resource Languages (LoResLM 2025)}}\\ 
\noindent{\emph{Organizers: Hansi Hettiarachchi, Tharindu Ranasinghe, Paul Rayson, Ruslan Mitkov, Mohamed Gaber, Damith Premasiri, Fiona Anting Tan, and Lasitha Uyangodage}}\\

\noindent{Neural language models have revolutionised natural language processing (NLP) and have provided state-of-the-art results for many tasks. However, their effectiveness is largely dependent on the pre-training resources. Therefore, language models (LMs) often struggle with low-resource languages in both training and evaluation. Recently, there has been a growing trend in developing and adopting LMs for low-resource languages. This workshop aims to provide a forum for researchers to share and discuss their ongoing work on LMs for low-resource languages.} \\

\noindent{\url{https://loreslm.github.io/}} \\
\noindent{Time: 08:45-18:00}\\
\noindent{Location: Capital Suite 5 (Level 2)}\\

\noindent\hrulefill


\noindent{\textbf{Workshop 17: The 1st International Workshop on Nakba Narratives as Language Resources (Nakba-NLP 2025)}}\\ 
\noindent{\emph{Organizers: Mustafa Jarrar, Nizar Habash, Mo El-Haj, Amal Haddad, Zeina Jallad, Camille Mansour, Diana Allan, Paul Rayson, and Tymaa Hammouda}}\\

\noindent{The narratives of the (ongoing) Palestinian Nakba possess significant historical, cultural, literary, and academic value. Preserving this content and empowering it with AI tools is crucial for ensuring its accessibility and usability for present and future generations. Nakba narratives and testimonies exist in diverse formats such as manuscripts, books, audio recordings, novels, and films. Converting this content into a machine-understandable format presents a notable challenge. Establishing accessible archives and well-annotated collections is essential for researchers and historians to verify and share meaningful information. This workshop aims to explore how artificial intelligence, natural language processing, and corpus linguistics can assist in understanding, disseminating and preserving, Nakba narratives and testimonies. The goal is to create accessible, comprehensive, and well-annotated collections that empower researchers and historians to validate and share critical insights derived from these data. The workshop targets datasets and narratives in Arabic, English, and other languages, however, submitted articles should be written in English.} \\

\noindent{\url{https://sina.birzeit.edu/nakba-nlp/}} \\
\noindent{Time: 09:00-17:15}\\
\noindent{Location: Online Virtual Only}\\

\noindent\hrulefill


\noindent{\textbf{Workshop 18: RegNLP-2025: The First Workshop on Regulatory NLP}}\\ 
\noindent{\emph{Organizers: Annie Antón, Barry West, Tuba Gokhan, Kexin Wang, Iryna Gurevych, and Ted Briscoe}}\\

% In addition, we are hosting the RIRAG (Regulatory Information Retrieval and Answer Generation) shared task, which focuses on advancing methods for regulatory compliance through information retrieval and answer generation.

% This workshop seeks to share current findings, discuss challenges, and identify promising directions for future research. Most importantly, it aims to foster a collaborative community dedicated to advancing NLP applications in the regulatory domain at this critical juncture.

\noindent{The complexity, volume, and ever-changing nature of regulatory documents present unique challenges in governance, compliance, and legal frameworks across various sectors. Addressing these challenges demands specialized approaches in natural language processing (NLP) to enable effective management and utilization of regulatory content. Recent advancements in NLP have opened new avenues for tackling these issues, specifically tailored to the domain of regulatory documents. These advancements include sophisticated techniques for document parsing, entity recognition, and automated compliance checking, which are essential for navigating the intricate landscape of regulatory requirements. Despite these technological strides, significant open questions and challenges remain. How can NLP models better handle the dynamic and diverse nature of regulatory texts? What methods are most effective for extracting and synthesizing information from vast and complex document repositories? How can we ensure the accuracy and reliability of automated compliance tools? Moreover, what are the best practices for adapting NLP models to the highly specialized language and context of regulatory documents?
The first workshop on Regulatory Natural Language Processing (RegNLP) aims to convene a diverse group of researchers and practitioners from NLP, legal informatics, compliance, and related fields to explore these questions.} \\

\noindent{\url{https://regnlp.github.io/}} \\
\noindent{Time: 09:00-17:30}\\
\noindent{Location: Capital Suite 13 (Level 2)}\\

\noindent\hrulefill


\noindent{\textbf{Workshop 19: The Second Workshop on South East Asian Language Processing (SEALP)}}\\ 
\noindent{\emph{Organizers: Derry Wijaya, Alham Fikri Aji, Clara Vania, Genta Indra Winata, Ayu Purwarianti}}\\

\noindent{South East Asia is one of the most linguistically diverse regions in the world, with over 1200 languages spoken by 680 million people. However, the diversity of South East Asian languages has long been at risk due to the emphasis on national languages as lingua franca in South East Asian countries at the end of colonization; and the increasing prominence of English due to the necessities of globalization. This workshop will bring together practitioners from academia, government, and industry interested in the research and development of language technologies for SEA languages. The workshop also aims to build an inclusive community of everyone passionate about SEA languages, increase community awareness of works that have been developed to date on these languages, and foster collaborations that will strengthen and spur NLP research and development in SEA languages.} \\

\noindent{\url{https://sealp-workshop.github.io/}} \\
\noindent{Time: Check online for Schedule }\\
\noindent{Location: Online Virtual Only }\\

\noindent\hrulefill


\noindent{\textbf{Workshop 20: SUMEval-2: The 2nd Workshop on Scaling Up Multilingual \& Multi-Cultural Evaluation}}\\ 
\noindent{\emph{Organizers: Hellina Hailu Nigatu, Monojit Choudhury, Oana Ignat, Sebastian Ruder, Sunayana Sitaram, and Vishrav Chaudhary}}\\

\noindent{Massively Multilingual Language Models (MMLMs) like mBERT, XLMR and XY-LENT support around 100 languages of the world. Additionally, generative models like GPT-4 and BLOOM are getting attention from the NLP community and the public. However, most existing multilingual NLP benchmarks reflect a handful of cultures and languages. The languages present in evaluation benchmarks are usually high-resource and largely belong to the Indo-European language family. By extension, the cultures represented in evaluation benchmarks are also largely reflective of Western society. This makes current evaluation unreliable and does not provide a full picture of the performance of MMLMs across the linguistic and cultural landscape. Although efforts are being made to create benchmarks that cover a larger variety of tasks, cultures, languages, and language families, it is unlikely that we will be able to build benchmarks covering all languages and cultures. Due to this, there is recent interest in alternate strategies for evaluating MMLMs, including performance prediction and Machine Translation of test data.} \\

\noindent{\url{https://sites.google.com/view/sumeval-2025}} \\
\noindent{Time: 09:00-17:30}\\
\noindent{Location: Capital Suite 6 (Level 2)}\\

\noindent\hrulefill


\noindent{\textbf{Workshop 21: The 4th Workshop on Arabic Corpus Linguistics (WACL-4)}}\\ 
\noindent{\emph{Organizers: Saad Ezzini, Hamza Alami, Ismail Berrada, Abdessamad Benlahbib, Abdelkader El Mahdaouy, Salima Lamsiyah, Hatim Derrouz, Amal Haddad, Mustafa Jarrar, Mo El-Haj, Ruslan Mitkov, and Paul Rayson}}\\

\noindent{The field of Arabic language research using corpora and corpus methods has experienced significant growth and development in recent years. What once were isolated efforts have now transformed into a vibrant and expansive area of study, advancing rapidly across multiple dimensions in both corpus and computational linguistics. Building upon the success of previous editions—WACL-1 in 2011, WACL-2 in 2013 in conjunction with the Corpus Linguistics Conference at Lancaster University, and WACL-3 in 2019 at the Corpus Linguistics 2019 conference at Cardiff University—we are excited to announce the fourth edition of the Workshop on Arabic Corpus Linguistics (WACL-4). The primary objectives of WACL-4 are to highlight the latest developments in the creation, annotation, and application of Arabic corpora, including the introduction of new corpora and advancements in annotation techniques, while fostering collaboration among researchers from diverse institutions and regions to stimulate joint research projects and interdisciplinary initiatives. This edition will place a special emphasis on the study of Arabic dialects, including non-standard and regional varieties, to broaden the understanding of Arabic in its various manifestations and support research on under-resourced linguistic varieties. Additionally, WACL-4 aims to encourage the development and refinement of Natural Language Processing (NLP) systems and tools tailored for Arabic, integrating corpora into NLP workflows, creating new computational tools, and evaluating existing systems to improve their efficacy in processing Arabic text.} \\

\noindent{\url{https://wp.lancs.ac.uk/wacl4/}} \\
\noindent{Time: 09:00-16:30}\\
\noindent{Location: Online Virtual Only}\\

\noindent\hrulefill


\noindent{\textbf{Workshop 22: Writing Aids at the Crossroads of AI, Cognitive Science and NLP WR-AI-CogS}}\\ 
\noindent{\emph{Organizers: Michael ZOCK, Kentaro Inui, and Zheng Yuan}}\\

\noindent{ This workshop is dedicated to developing writing aids that align with human cognition, including attention and memory limitations, common writing habits, knowledge states, and information needs. In other words, we focus on the cognitive and engineering aspects of interactive writing. Our goal is not only to help people acquire and improve their writing skills but also to enhance their productivity. By leveraging computer technology, we aim to enable them to produce better texts in less time.} \\

\noindent{\url{https://sites.google.com/view/wraicogs1}} \\
\noindent{Time: 09:00-18:30}\\
\noindent{Location: Capital Suite 8 (Level 2)}\\